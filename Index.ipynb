{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"zLCTeDm-jLfS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705299216084,"user_tz":300,"elapsed":2352,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"}},"outputId":"dac6d432-8e76-48cc-f647-1ee3c05f2c22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","drive  sample_data\n"]}],"source":["#mount drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","!ls"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166,"status":"ok","timestamp":1705299217940,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"},"user_tz":300},"id":"AnaSX0XUjLfV","outputId":"9a27cce8-35a7-4e85-ab60-4e890d52f889"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Personal-Projects/crop-damage-classification\n","common\t     dataloading  Index.ipynb  output\t\t    project-structure.md  transforms\n","config.yaml  experiments  index.py     preprocess\t    README.md\t\t  visualization\n","data\t     Index_bc.py  models       preprocess_input.py  run.yaml\n"]}],"source":["# move into project directory\n","repo_name = \"crop-damage-classification\"\n","%cd /content/drive/MyDrive/Personal-Projects/$repo_name\n","!ls"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":196,"status":"ok","timestamp":1705299223762,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"},"user_tz":300},"id":"qVSyI39BjLfW","outputId":"5fb3705a-5827-4455-d0f8-af725f31a85d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n!pip install matplotlib numpy pandas pyyaml opencv-python\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["# set up environment\n","# comment if not required\n","'''\n","!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!pip install matplotlib numpy pandas pyyaml opencv-python\n","'''"]},{"cell_type":"markdown","metadata":{"id":"uZ3v5itpj3ae"},"source":["# Following cells are for downloading data"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41237,"status":"ok","timestamp":1705290894628,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"},"user_tz":300},"id":"1Ox_E5YpjLfW","outputId":"4add4524-9ad6-4178-ba4d-de3c9364855c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.34.19)\n","Requirement already satisfied: botocore<1.35.0,>=1.34.19 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.34.19)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n","Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.10.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.19->boto3) (2.8.2)\n","Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.19->boto3) (2.0.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.19->boto3) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"]}],"source":["# this cell is for downloading data.\n","# as of yet data is not hosted and is available in the private data folder\n","# comment if not required\n","!pip install boto3\n","!pip install tqdm"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":11327,"status":"ok","timestamp":1705299239189,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"},"user_tz":300},"id":"kwOcWXgAjLfW"},"outputs":[],"source":["# setup some imports\n","#custom imports\n","from transforms.transforms import ToTensor, Resize, CenterCrop\n","from dataloading.dataset import CropDataset\n","from common.utils import get_exp_params, init_config, get_config, save2config, get_modelinfo, get_saved_model, read_json\n","from models.resnet18 import Resnet18\n","from models.custom_models import get_model\n","from experiments.experiments import Experiment\n","from visualization.visualization import Visualization\n","from experiments.test_model import ModelTester\n","from preprocess.preprocessor import Preprocessor\n","\n","#py imports\n","import random\n","import numpy as np\n","import os\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"i_Jc81A_kF4o","executionInfo":{"status":"ok","timestamp":1705290910354,"user_tz":300,"elapsed":776,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"}}},"outputs":[],"source":["import boto3\n","from pathlib import Path\n","from botocore import UNSIGNED\n","from botocore.client import Config\n","from tqdm.notebook import tqdm\n","\n","def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n","    file_names = []\n","    folders = []\n","\n","    default_kwargs = {\n","        \"Bucket\": bucket_name,\n","        \"Prefix\": prefix\n","    }\n","    next_token = \"\"\n","\n","    while next_token is not None:\n","        updated_kwargs = default_kwargs.copy()\n","        if next_token != \"\":\n","            updated_kwargs[\"ContinuationToken\"] = next_token\n","\n","        response = s3_client.list_objects_v2(**updated_kwargs)\n","        contents = response.get(\"Contents\")\n","\n","        for result in contents:\n","            key = result.get(\"Key\")\n","            if key[-1] == \"/\":\n","                folders.append(key)\n","            else:\n","                file_names.append(key)\n","\n","        next_token = response.get(\"NextContinuationToken\")\n","\n","    return file_names, folders\n","\n","def download_files(s3_client, bucket_name, local_path, file_names, folders):\n","    local_path = Path(local_path)\n","\n","    for folder in tqdm(folders):\n","        folder_path = Path.joinpath(local_path, folder)\n","\t\t\t\t# Create all folders in the path\n","        folder_path.mkdir(parents=True, exist_ok=True)\n","\n","    for file_name in tqdm(file_names):\n","        file_path = Path.joinpath(local_path, file_name)\n","\t\t\t\t# Create folder for parent directory\n","        file_path.parent.mkdir(parents=True, exist_ok=True)\n","        s3_client.download_file(\n","            bucket_name,\n","            file_name,\n","            str(file_path)\n","        )\n","\n","data_path = 'data/input/images'\n","if not(os.path.exists(os.path.join(os.getcwd(), data_path))):\n","    client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n","    file_names, folders = get_file_folders(client, 'cgiar-crop-damage-classification-challenge')\n","    download_files(\n","        client,\n","        'cgiar-crop-damage-classification-challenge',\n","        \"/content/drive/MyDrive/Personal-Projects/crop-damage-classification/data/input\",\n","        file_names,\n","        folders\n","    )"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1705299239190,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"},"user_tz":300},"id":"swtjFj_LjLfX","outputId":"496e11af-14d4-46b7-a990-733ba03a1f8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Config parameters\n","\n","{'X_key': 'image', 'data_dir': '/content/drive/MyDrive/Personal-Projects/crop-damage-classification/data', 'device': 'cpu', 'img_dir': '/content/drive/MyDrive/Personal-Projects/crop-damage-classification/data/input/images', 'output_dir': '/content/drive/MyDrive/Personal-Projects/crop-damage-classification/output', 'root_dir': '/content/drive/MyDrive/Personal-Projects/crop-damage-classification', 'use_gpu': False, 'y_key': 'label'}\n"]}],"source":["# initialize directories and config data\n","init_config()\n","config = get_config()\n","print('Config parameters\\n')\n","print(config)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1705299239191,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"},"user_tz":300},"id":"b5bdgR1sjLfX","outputId":"a0da2167-0595-456c-a072-9a2e3b3fc924"},"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment parameters\n","\n","{'transform': {'resize_dim': 256, 'crop_dim': 224}, 'train': {'shuffle_data': True, 'batch_size': 128, 'val_split_method': 'k-fold', 'k': 5, 'val_percentage': 20, 'loss': 'cross-entropy', 'batch_interval': 512, 'epoch_interval': 1, 'num_epochs': 10}, 'model': {'name': 'resnet18', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 1e-05, 'amsgrad': False, 'momentum': 0.9}, 'test_model': False}\n"]}],"source":["# read experiment parameters\n","exp_params = get_exp_params()\n","print('Experiment parameters\\n')\n","print(exp_params)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705299239192,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"},"user_tz":300},"id":"GEnkN2-_jLfY"},"outputs":[],"source":["#initialize randomness seed\n","seed = 123\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"qS5oBZb9jLfY","executionInfo":{"status":"ok","timestamp":1705299240102,"user_tz":300,"elapsed":5,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"}}},"outputs":[],"source":["#preprocess data or load preprocessed data\n","\n","#build label dict\n","label_dict = {\n","    'DR': 0,\n","    'G': 1,\n","    'ND': 2,\n","    'WD': 3,\n","    'other': 4\n","}\n","\n","class_dict = {\n","    0: 'DR',\n","    1: 'G',\n","    2: 'ND',\n","    3: 'WD',\n","    4: 'other'\n","}"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gu_4BlM-UCkr","executionInfo":{"status":"ok","timestamp":1705299242060,"user_tz":300,"elapsed":259,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"}},"outputId":"8fdcd96a-f1a0-4c4a-950b-ee63a2d0b81c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Full train dataset length: 26068\n","Test dataset length: 8663\n","Subset train dataset length: 260 \n","\n"]}],"source":["#save X_key and y_key\n","save2config('X_key', 'image')\n","save2config('y_key', 'label')\n","\n","#transform data\n","data_transforms = transforms.Compose([ToTensor(), Resize(exp_params['transform']['resize_dim']), CenterCrop(exp_params['transform']['crop_dim'])])\n","\n","#convert to dataset\n","ftr_dataset = CropDataset('input/Train.csv', label_dict, False, transforms=data_transforms)\n","test_dataset = CropDataset('input/Test.csv', label_dict, True, transforms=data_transforms)\n","smlen = int(0.01 * len(ftr_dataset))\n","smftr_dataset = torch.utils.data.Subset(ftr_dataset, list(range(smlen)))\n","print('Full train dataset length:', len(ftr_dataset))\n","print('Test dataset length:', len(test_dataset))\n","print('Subset train dataset length:', smlen, '\\n')\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"i-YdanntUCks","executionInfo":{"status":"ok","timestamp":1705285652040,"user_tz":300,"elapsed":202,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"}},"colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"12bcfdca-470b-474a-97f4-afc793e31e03"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n#running experiment on small subset of the dataset\\nexp = Experiment(exp_params['model']['name'], smftr_dataset)\\nmodel_history = exp.train()\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["'''\n","#running experiment on small subset of the dataset\n","exp = Experiment(exp_params['model']['name'], smftr_dataset)\n","model_history = exp.train()\n","'''"]},{"cell_type":"code","source":["'''\n","print(\"Getting metrics for small data\")\n","preop = Preprocessor()\n","all_folds_metrics = preop.get_dataset_metrics(smftr_dataset, exp_params['train']['val_split_method'])\n","print(all_folds_metrics)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9oYnMlWAtKHc","executionInfo":{"status":"ok","timestamp":1705286993845,"user_tz":300,"elapsed":1095093,"user":{"displayName":"Vinitra Mk","userId":"02676117449369578011"}},"outputId":"17f2cebb-5458-4beb-c6df-09b78a0b4194"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Getting metrics for small data\n","[52, 104, 156, 208]\n","\tCalculating metric for split 0 starting with 0, ending with 52\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["si 0\n","si 52 52\n","\tCalculating metric for split 1 starting with 52, ending with 104\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["si 52\n","si 104 104\n","\tCalculating metric for split 2 starting with 104, ending with 156\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["si 104\n","si 156 156\n","\tCalculating metric for split 3 starting with 156, ending with 208\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["si 156\n","si 208 208\n","{0: ([0.4691032844499407, 0.4642999397722259, 0.3367527026156445], [0.25391136811674037, 0.25511066727296733, 0.29634439087307474], [0.2539114585104833, 0.2551107573869328, 0.2963444952522965]), 1: ([0.4693464024142023, 0.4647783213452792, 0.33516081830332584], [0.25097124265802273, 0.2509423582253417, 0.2935421279130286], [0.2509713321361989, 0.2509424470412791, 0.2935422314196143]), 2: ([0.46598202827447943, 0.4625296299462866, 0.3345988147630704], [0.25433435014241607, 0.2563203380297623, 0.2973605209886066], [0.2543344406678774, 0.25632042852042414, 0.29736062568425525]), 3: ([0.47045614981483663, 0.46831747207811697, 0.34043276211154105], [0.2549591611211113, 0.2553095468361302, 0.2982085550849592], [0.2549592518411412, 0.2553096370120277, 0.29820866004468904])}\n"]}]},{"cell_type":"code","source":["print(\"Getting metrics for data\")\n","preop = Preprocessor()\n","all_folds_metrics = preop.get_dataset_metrics(ftr_dataset, exp_params['train']['val_split_method'])\n","print(all_folds_metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NKroKRypQgkc","outputId":"67dde65a-2f2b-4e54-9bd3-ed773ef88ba6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Getting metrics for data\n","\tCalculating metric for split 0 starting with 0, ending with 5213\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#model training on full dataset\n","exp = Experiment(exp_params['model']['name'], ftr_dataset)\n","model_history = exp.train()"],"metadata":{"id":"LeBlN4k7VZFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Getting metrics for data\")\n","preop = Preprocessor()\n","all_folds_metrics = preop.get_dataset_metrics(ftr_dataset, exp_params['train']['val_split_method'])\n","print(all_folds_metrics)"],"metadata":{"id":"mG191kJv5pME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get best model\n","model = get_model(exp_params[\"model\"][\"name\"])\n","model = get_saved_model(model, '')\n","model_info = get_modelinfo('')\n","best_fold = model_info['fold']\n","metric_path = os.path.join(config[\"root_dir\"], \"models/checkpoints/all_folds_metrics.json\")\n","all_folds_metrics = read_json(metric_path)\n","print('All folds metrics')\n","print(all_folds_metrics)\n","\n","print(\"\\nModel validation results\")\n","print(model_info['results']['trlosshistory'])\n","#visualization results\n","vis = Visualization(model_info, model_history)\n","vis.get_results()"],"metadata":{"id":"uuSnMkZmnDrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model testing\n","print(\"\\n\\nTesting Saved Model\")\n","mt = ModelTester(model, test_dataset, all_folds_metrics[best_fold])\n","mt.test_and_save_csv(class_dict)"],"metadata":{"id":"pvqYKcgPnm9r"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}