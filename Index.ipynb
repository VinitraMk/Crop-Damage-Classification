{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"zLCTeDm-jLfS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705272020803,"user_tz":300,"elapsed":3503,"user":{"displayName":"Vinitra","userId":"09606499732062179579"}},"outputId":"20f992fd-f109-4a32-99b7-4270a054abdb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","drive  sample_data\n"]}],"source":["#mount drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","!ls"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":553,"status":"ok","timestamp":1705272021351,"user":{"displayName":"Vinitra","userId":"09606499732062179579"},"user_tz":300},"id":"AnaSX0XUjLfV","outputId":"a25c0bbb-b801-4c4e-b4eb-7ed238b77b02"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Personal-Projects/crop-damage-classification\n","common\t     dataloading  Index.ipynb  output\t\t    project-structure.md  transforms\n","config.yaml  experiments  index.py     preprocess\t    README.md\t\t  visualization\n","data\t     Index_bc.py  models       preprocess_input.py  run.yaml\n"]}],"source":["# move into project directory\n","repo_name = \"crop-damage-classification\"\n","%cd /content/drive/MyDrive/Personal-Projects/$repo_name\n","!ls"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1705272021351,"user":{"displayName":"Vinitra","userId":"09606499732062179579"},"user_tz":300},"id":"qVSyI39BjLfW","outputId":"4fd40aa6-0600-4731-86e0-a557f95058a2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n!pip install matplotlib numpy pandas pyyaml opencv-python\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["# set up environment\n","# comment if not required\n","'''\n","!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!pip install matplotlib numpy pandas pyyaml opencv-python\n","'''"]},{"cell_type":"markdown","metadata":{"id":"uZ3v5itpj3ae"},"source":["# Following cells are for downloading data"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25624,"status":"ok","timestamp":1705272046960,"user":{"displayName":"Vinitra","userId":"09606499732062179579"},"user_tz":300},"id":"1Ox_E5YpjLfW","outputId":"6bbd5fd2-7506-464d-bf43-8e6815b37727"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.34.19)\n","Requirement already satisfied: botocore<1.35.0,>=1.34.19 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.34.19)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n","Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.10.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.19->boto3) (2.8.2)\n","Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.19->boto3) (2.0.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.19->boto3) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"]}],"source":["# this cell is for downloading data.\n","# as of yet data is not hosted and is available in the private data folder\n","# comment if not required\n","!pip install boto3\n","!pip install tqdm"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8178,"status":"ok","timestamp":1705272055132,"user":{"displayName":"Vinitra","userId":"09606499732062179579"},"user_tz":300},"id":"kwOcWXgAjLfW"},"outputs":[],"source":["# setup some imports\n","#custom imports\n","from transforms.transforms import ToTensor, Resize, CenterCrop\n","from dataloading.dataset import CropDataset\n","from common.utils import get_exp_params, init_config, get_config, save2config, get_modelinfo, get_saved_model\n","from models.resnet18 import Resnet18\n","from models.custom_models import get_model\n","from experiments.experiments import Experiment\n","from visualization.visualization import Visualization\n","from experiments.test_model import ModelTester\n","\n","#py imports\n","import random\n","import numpy as np\n","import os\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"i_Jc81A_kF4o","executionInfo":{"status":"ok","timestamp":1705272055132,"user_tz":300,"elapsed":10,"user":{"displayName":"Vinitra","userId":"09606499732062179579"}}},"outputs":[],"source":["import boto3\n","from pathlib import Path\n","from botocore import UNSIGNED\n","from botocore.client import Config\n","from tqdm.notebook import tqdm\n","\n","def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n","    file_names = []\n","    folders = []\n","\n","    default_kwargs = {\n","        \"Bucket\": bucket_name,\n","        \"Prefix\": prefix\n","    }\n","    next_token = \"\"\n","\n","    while next_token is not None:\n","        updated_kwargs = default_kwargs.copy()\n","        if next_token != \"\":\n","            updated_kwargs[\"ContinuationToken\"] = next_token\n","\n","        response = s3_client.list_objects_v2(**updated_kwargs)\n","        contents = response.get(\"Contents\")\n","\n","        for result in contents:\n","            key = result.get(\"Key\")\n","            if key[-1] == \"/\":\n","                folders.append(key)\n","            else:\n","                file_names.append(key)\n","\n","        next_token = response.get(\"NextContinuationToken\")\n","\n","    return file_names, folders\n","\n","def download_files(s3_client, bucket_name, local_path, file_names, folders):\n","    local_path = Path(local_path)\n","\n","    for folder in tqdm(folders):\n","        folder_path = Path.joinpath(local_path, folder)\n","\t\t\t\t# Create all folders in the path\n","        folder_path.mkdir(parents=True, exist_ok=True)\n","\n","    for file_name in tqdm(file_names):\n","        file_path = Path.joinpath(local_path, file_name)\n","\t\t\t\t# Create folder for parent directory\n","        file_path.parent.mkdir(parents=True, exist_ok=True)\n","        s3_client.download_file(\n","            bucket_name,\n","            file_name,\n","            str(file_path)\n","        )\n","\n","data_path = 'data/input/images'\n","if not(os.path.exists(os.path.join(os.getcwd(), data_path))):\n","    client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n","    file_names, folders = get_file_folders(client, 'cgiar-crop-damage-classification-challenge')\n","    download_files(\n","        client,\n","        'cgiar-crop-damage-classification-challenge',\n","        \"/content/drive/MyDrive/Personal-Projects/crop-damage-classification/data/input\",\n","        file_names,\n","        folders\n","    )"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1705272055474,"user":{"displayName":"Vinitra","userId":"09606499732062179579"},"user_tz":300},"id":"swtjFj_LjLfX","outputId":"4278ef97-28b0-4cd9-9513-ed9af4524d68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Config parameters\n","\n","{'X_key': 'image', 'data_dir': '/content/drive/MyDrive/Personal-Projects/crop-damage-classification/data', 'device': 'cuda', 'img_dir': '/content/drive/MyDrive/Personal-Projects/crop-damage-classification/data/input/images', 'output_dir': '/content/drive/MyDrive/Personal-Projects/crop-damage-classification/output', 'root_dir': '/content/drive/MyDrive/Personal-Projects/crop-damage-classification', 'use_gpu': True, 'y_key': 'label'}\n"]}],"source":["# initialize directories and config data\n","init_config()\n","config = get_config()\n","print('Config parameters\\n')\n","print(config)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1705272055474,"user":{"displayName":"Vinitra","userId":"09606499732062179579"},"user_tz":300},"id":"b5bdgR1sjLfX","outputId":"825c8cf7-5765-4cc1-9144-6731a87238a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment parameters\n","\n","{'transform': {'resize_dim': 256, 'crop_dim': 224}, 'train': {'shuffle_data': True, 'batch_size': 128, 'val_split_method': 'k-fold', 'k': 5, 'val_percentage': 20, 'loss': 'cross-entropy', 'batch_interval': 512, 'epoch_interval': 1, 'num_epochs': 10}, 'model': {'name': 'resnet18', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 1e-05, 'amsgrad': False, 'momentum': 0.9}, 'test_model': False}\n"]}],"source":["# read experiment parameters\n","exp_params = get_exp_params()\n","print('Experiment parameters\\n')\n","print(exp_params)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1705272055474,"user":{"displayName":"Vinitra","userId":"09606499732062179579"},"user_tz":300},"id":"GEnkN2-_jLfY"},"outputs":[],"source":["#initialize randomness seed\n","seed = 123\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qS5oBZb9jLfY","executionInfo":{"status":"ok","timestamp":1705272055475,"user_tz":300,"elapsed":10,"user":{"displayName":"Vinitra","userId":"09606499732062179579"}}},"outputs":[],"source":["#preprocess data or load preprocessed data\n","\n","#build label dict\n","label_dict = {\n","    'DR': 0,\n","    'G': 1,\n","    'ND': 2,\n","    'WD': 3,\n","    'other': 4\n","}\n","\n","class_dict = {\n","    0: 'DR',\n","    1: 'G',\n","    2: 'ND',\n","    3: 'WD',\n","    4: 'other'\n","}"]},{"cell_type":"code","source":["import os\n","from common.utils import get_config\n","import pandas as pd\n","from PIL import Image\n","import numpy as np\n","import torch\n","from torchvision.io import read_image\n","\n","class Preprocessor:\n","\n","    def __init__(self, data_filesuffix = 224):\n","        cfg = get_config()\n","        self.image_dir = cfg[\"img_dir\"]\n","        self.train_labels = pd.read_csv(os.path.join(cfg[\"data_dir\"], \"input/Train.csv\"))\n","        self.test_labels = pd.read_csv(os.path.join(cfg[\"data_dir\"], \"input/Test.csv\"))\n","        self.processed_img_dir = os.path.join(cfg[\"data_dir\"], \"processed_input\")\n","        self.X_key = cfg['X_key']\n","        self.y_key = cfg['y_key']\n","        self.data_filesuffix = data_filesuffix\n","\n","\n","    def transform_input(self, transform):\n","        train_files = self.train_labels[\"filename\"].tolist()\n","        test_files = self.test_labels[\"filename\"].tolist()\n","        train_data = np.empty((1, self.data_filesuffix, self.data_filesuffix, 3))\n","        test_data = np.empty((1, self.data_filesuffix, self.data_filesuffix, 3))\n","        print('Iterating through train files')\n","        for i, tf in enumerate(train_files):\n","            img = read_image(os.path.join(self.image_dir, tf))\n","            sample = { self.X_key: img }\n","            img = transform(sample)[self.X_key]\n","            img = img.cpu().detach().numpy()\n","            img = np.transpose(img, (1, 2, 0))\n","            train_data = np.concatenate((train_data, np.expand_dims(img, 0)))\n","        train_data = train_data[1:]\n","        np.savez_compressed(os.path.join(self.processed_img_dir, f\"train_{self.data_filesuffix}\"),  train_data)\n","        del train_data\n","        print('\\nIterating through test files')\n","        for i, tf in enumerate(test_files):\n","            img = read_image(os.path.join(self.image_dir, tf))\n","            sample = { self.X_key: img }\n","            img = transform(sample)[self.X_key]\n","            img = img.cpu().detach().numpy()\n","            img = np.transpose(img, (1, 2, 0))\n","            test_data = np.concatenate((test_data, np.expand_dims(img, 0)))\n","        test_data = test_data[1:]\n","        np.savez_compressed(os.path.join(self.processed_img_dir, f\"test_{self.data_filesuffix}\"), test_data)\n","        del test_data\n","\n","    def get_dataset_metrics(self, dataset):\n","        dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False)\n","        print(len(dataloader))\n","        pop_mean = []\n","        pop_std0 = []\n","        pop_std1 = []\n","        for i, data in enumerate(dataloader):\n","            # shape (batch_size, 3, height, width)\n","            numpy_image = data[self.X_key].numpy()\n","            print(numpy_image.shape)\n","            # shape (3,)\n","            batch_mean = np.mean(numpy_image, axis=(0,2,3))\n","            batch_std0 = np.std(numpy_image, axis=(0,2,3))\n","            batch_std1 = np.std(numpy_image, axis=(0,2,3), ddof=1)\n","\n","            del data[self.X_key]\n","\n","            pop_mean.append(batch_mean)\n","            pop_std0.append(batch_std0)\n","            pop_std1.append(batch_std1)\n","\n","        # shape (num_iterations, 3) -> (mean across 0th axis) -> shape (3,)\n","        pop_mean = np.array(pop_mean).mean(axis=0)\n","        pop_mean = [x/255 for x in pop_mean]\n","        pop_std0 = np.array(pop_std0).mean(axis=0)\n","        pop_std0 = [x/255 for x in pop_std0]\n","        pop_std1 = np.array(pop_std1).mean(axis=0)\n","        pop_std1 = [x/255 for x in pop_std1]\n","        return pop_mean, pop_std0, pop_std1\n","\n","    def make_label_csv(self):\n","        ## This function is for implementing code that constructs a csv file\n","        ## listing labels of all images. The csv file will have 4 columsn - image file name, label (encoded),\n","        ## original label and full path\n","        pass\n","\n"],"metadata":{"id":"w3ANGqChzj8T","executionInfo":{"status":"ok","timestamp":1705272055475,"user_tz":300,"elapsed":10,"user":{"displayName":"Vinitra","userId":"09606499732062179579"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gu_4BlM-UCkr","executionInfo":{"status":"ok","timestamp":1705272055476,"user_tz":300,"elapsed":10,"user":{"displayName":"Vinitra","userId":"09606499732062179579"}},"outputId":"a4314a5a-b04a-4afa-842b-57c1314b5913"},"outputs":[{"output_type":"stream","name":"stdout","text":["Full train dataset length: 26068\n","Test dataset length: 8663\n","Subset train dataset length: 260 \n","\n"]}],"source":["#save X_key and y_key\n","save2config('X_key', 'image')\n","save2config('y_key', 'label')\n","\n","#transform data\n","data_transforms = transforms.Compose([ToTensor(), Resize(exp_params['transform']['resize_dim']), CenterCrop(exp_params['transform']['crop_dim'])])\n","\n","#convert to dataset\n","ftr_dataset = CropDataset('input/Train.csv', label_dict, False, transforms=data_transforms)\n","test_dataset = CropDataset('input/Test.csv', label_dict, True, transforms=data_transforms)\n","smlen = int(0.01 * len(ftr_dataset))\n","smftr_dataset = torch.utils.data.Subset(ftr_dataset, list(range(smlen)))\n","print('Full train dataset length:', len(ftr_dataset))\n","print('Test dataset length:', len(test_dataset))\n","print('Subset train dataset length:', smlen, '\\n')\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"i-YdanntUCks","executionInfo":{"status":"ok","timestamp":1705272055476,"user_tz":300,"elapsed":10,"user":{"displayName":"Vinitra","userId":"09606499732062179579"}},"colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"bfee32e4-bda3-44b7-dae4-f587bc527101"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n#running experiment on small subset of the dataset\\nexp = Experiment(exp_params['model']['name'], smftr_dataset)\\nmodel_history = exp.train()\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["'''\n","#running experiment on small subset of the dataset\n","exp = Experiment(exp_params['model']['name'], smftr_dataset)\n","model_history = exp.train()\n","'''"]},{"cell_type":"code","source":["#model training on full dataset\n","exp = Experiment(exp_params['model']['name'], ftr_dataset)\n","model_history = exp.train()"],"metadata":{"id":"LeBlN4k7VZFY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"10fc8c41-215a-4ca2-aa80-e273541820af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running split 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["(128, 3, 224, 224)\n","(128, 3, 224, 224)\n","(128, 3, 224, 224)\n","(128, 3, 224, 224)\n","(128, 3, 224, 224)\n","(128, 3, 224, 224)\n","(128, 3, 224, 224)\n","(128, 3, 224, 224)\n","(128, 3, 224, 224)\n","(128, 3, 224, 224)\n","(128, 3, 224, 224)\n","(128, 3, 224, 224)\n"]}]},{"cell_type":"code","source":["# get best model\n","model = get_model(exp_params[\"model\"][\"name\"])\n","model = get_saved_model(model, '')\n","model_info = get_modelinfo('')\n","\n","print(\"\\nModel validation results\")\n","print(model_info['results']['trlosshistory'])\n","#visualization results\n","vis = Visualization(model_info, model_history)\n","vis.get_results()"],"metadata":{"id":"uuSnMkZmnDrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model testing\n","print(\"\\n\\nTesting Saved Model\")\n","mt = ModelTester(model, test_dataset)\n","mt.test_and_save_csv(class_dict)"],"metadata":{"id":"pvqYKcgPnm9r"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}